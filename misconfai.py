# -*- coding: utf-8 -*-
"""MisconfAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5uGN309vVZpL0BNbq0ZGD0iN_9Sl_BM

**Detail above the folder created and models**\
**In the Misconf folder**
1. rules.yaml --> is the rules files
2. Config_collection --> is the collection of configuration files with each category as subfolders
3. preprocessed_configs --> this is the preprocessed files
4. Rule_augmented_configs --> this have only rules defined vulnerabilities
5. Augmented_configs --> have the vulnerability as 1 to 3
6. Augmented_data --> have the detail about the augmented configs
7. Finalaugmented_configs --> have the configs with vulnerability from 1 to 5
8. Final_augmented data --> details about the above configs
9. Rule_analysis on rule --> we did the rule analysis on rule_augmented configs
10. Rule analysis on final --> rule analysis of final_augmented_configs
11. bert,roberta,codebert,electra,xlnet --> are the models we run with different evaluation files for each category
12. 2.bert, 2.roberta, 2.codebert, 2.electra, 2.xlnet --> are the models with fixed evaluation files per category.
13. metadata.csv --> csv generated from rule augmentation
14. finmetada.csv --> csv generated from final augmentation
15. All the augmentation files have the metedata.yaml for the details about the augmentation

**The flow is as follows:**

**Data Collection**

For web server
"""

import requests
import os
from google.colab import drive

# GitHub API Token
GITHUB_TOKEN = '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'
headers = {'Authorization': f'token {GITHUB_TOKEN}'}

# search query for common web server configs
queries = [
    "filename:httpd.conf",
    "filename:nginx.conf",
    "filename:apache2.conf",
    "filename:lighttpd.conf",
    "filename:caddyfile",
    "filename:cherokee.conf"
]

# Saving folder
save_dir = "/content/drive/MyDrive/MisconfAI/config_collection/web_servers"
os.makedirs(save_dir, exist_ok=True)

# Check for english
def is_english(text):
    try:
        ascii_text = text.encode('ascii')
        return True
    except UnicodeEncodeError:
        return False

# Fetching configuration files from GitHub
def fetch_files():
    for query in queries:
        print(f" Searching for: {query}")
        for page in range(1, 21):
            url = f"https://api.github.com/search/code?q={query}&per_page=100&page={page}"
            response = requests.get(url, headers=headers).json()
            items = response.get('items', [])

            if not items:
                break

            for item in items:
                repo_url = item['html_url'].replace('blob/', '').split('/')
                raw_url = f"https://raw.githubusercontent.com/{repo_url[3]}/{repo_url[4]}/main/{'/'.join(repo_url[6:])}"

                try:
                    file_data = requests.get(raw_url).text
                    if not is_english(file_data) or len(file_data.strip()) < 10:
                        continue

                    filename = "_".join(repo_url[3:]).replace("/", "_")
                    with open(os.path.join(save_dir, filename), 'w', encoding='utf-8') as f:
                        f.write(file_data)
                    print(f" Saved: {filename}")

                except Exception as e:
                    print(f" Error downloading {raw_url}: {e}")
                    continue

fetch_files()
print("\n Configuration file collection completed.")

"""Application"""

import requests
import os
from google.colab import drive

# GitHub API Token
GITHUB_TOKEN = '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'
headers = {'Authorization': f'token {GITHUB_TOKEN}'}

# Search queries for common application framework files
queries = [
    "filename:settings.py",
    "filename:application.yml",
    "filename:config.js",
    "filename:.env",
    "filename:config.py",
    "filename:application.properties",
    "filename:config/app.php",
]

# saving folder
save_dir = "/content/drive/MyDrive/MisconfAI/app_collection/"
os.makedirs(save_dir, exist_ok=True)

# List of already downloaded filenames
existing_files = set(os.listdir(save_dir))

# Check for english
def is_english(text):
    try:
        text.encode('ascii')
        return True
    except UnicodeEncodeError:
        return False

# Download
def fetch_files():
    for query in queries:
        print(f" Searching for: {query}")
        for page in range(1, 21):
            url = f"https://api.github.com/search/code?q={query}&per_page=100&page={page}"
            response = requests.get(url, headers=headers).json()
            items = response.get('items', [])

            if not items:
                break

            for item in items:
                try:
                    repo_url = item['html_url'].replace('blob/', '').split('/')
                    raw_url = f"https://raw.githubusercontent.com/{repo_url[3]}/{repo_url[4]}/main/{'/'.join(repo_url[6:])}"
                    filename = "_".join(repo_url[3:]).replace("/", "_")

                    if filename in existing_files:
                        continue

                    file_data = requests.get(raw_url).text
                    if not is_english(file_data) or len(file_data.strip()) < 10:
                        continue

                    with open(os.path.join(save_dir, filename), 'w', encoding='utf-8') as f:
                        f.write(file_data)

                    print(f" Saved: {filename}")
                    existing_files.add(filename)

                except Exception as e:
                    print(f" Error downloading {raw_url}: {e}")
                    continue

fetch_files()
print("\n Configuration file collection completed.")

"""Application"""

import requests
import os
from google.colab import drive

# GitHub API Token
GITHUB_TOKEN = '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'
headers = {'Authorization': f'token {GITHUB_TOKEN}'}

# search query for common application framework
queries = [
    "filename:settings.py",
    "filename:application.yml",
    "filename:config.js",
    "filename:.env",
    "filename:config.py",
    "filename:application.properties",
    "filename:config/app.php",
]

# Save folder in Google Drive
save_dir = "/content/drive/MyDrive/MisconfAI/app_collection/"
os.makedirs(save_dir, exist_ok=True)

# Check for english
def is_english(text):
    try:
        ascii_text = text.encode('ascii')
        return True
    except UnicodeEncodeError:
        return False

# Fetch configuration files from GitHub
def fetch_files():
    for query in queries:
        print(f" Searching for: {query}")
        for page in range(1, 21):
            url = f"https://api.github.com/search/code?q={query}&per_page=100&page={page}"
            response = requests.get(url, headers=headers).json()
            items = response.get('items', [])

            if not items:
                break

            for item in items:
                repo_url = item['html_url'].replace('blob/', '').split('/')
                raw_url = f"https://raw.githubusercontent.com/{repo_url[3]}/{repo_url[4]}/main/{'/'.join(repo_url[6:])}"

                try:
                    file_data = requests.get(raw_url).text
                    if not is_english(file_data) or len(file_data.strip()) < 10:
                        continue


                    filename = "_".join(repo_url[3:]).replace("/", "_")
                    with open(os.path.join(save_dir, filename), 'w', encoding='utf-8') as f:
                        f.write(file_data)
                    print(f"Saved: {filename}")

                except Exception as e:
                    print(f" Error downloading {raw_url}: {e}")
                    continue

fetch_files()
print("\n Configuration file collection completed.")

"""Application"""

import requests
import os
from google.colab import drive

# GitHub API Token
GITHUB_TOKEN = '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'
headers = {'Authorization': f'token {GITHUB_TOKEN}'}

# search query for common application framework
queries = [
    "filename:settings.py",
    "filename:application.yml",
    "filename:config.js",
    "filename:.env",
    "filename:config.py",
    "filename:application.properties",
    "filename:config/app.php",
]

# Save folder in Google Drive
save_dir = "/content/drive/MyDrive/MisconfAI/app_collection/"
os.makedirs(save_dir, exist_ok=True)

# Check for english
def is_english(text):
    try:
        ascii_text = text.encode('ascii')
        return True
    except UnicodeEncodeError:
        return False

# Fetch configuration files from GitHub
def fetch_files():
    for query in queries:
        print(f" Searching for: {query}")
        for page in range(1, 21):
            url = f"https://api.github.com/search/code?q={query}&per_page=100&page={page}"
            response = requests.get(url, headers=headers).json()
            items = response.get('items', [])

            if not items:
                break

            for item in items:
                repo_url = item['html_url'].replace('blob/', '').split('/')
                raw_url = f"https://raw.githubusercontent.com/{repo_url[3]}/{repo_url[4]}/main/{'/'.join(repo_url[6:])}"

                try:
                    file_data = requests.get(raw_url).text
                    if not is_english(file_data) or len(file_data.strip()) < 10:
                        continue


                    filename = "_".join(repo_url[3:]).replace("/", "_")
                    with open(os.path.join(save_dir, filename), 'w', encoding='utf-8') as f:
                        f.write(file_data)
                    print(f" Saved: {filename}")

                except Exception as e:
                    print(f" Error downloading {raw_url}: {e}")
                    continue

fetch_files()
print("\n Configuration file collection completed.")

"""Database"""

import requests
import os
from google.colab import drive

# GitHub API Token
GITHUB_TOKEN = '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'
headers = {'Authorization': f'token {GITHUB_TOKEN}'}

# search query for common databases
queries = [
    "filename:my.cnf",
    "filename:postgresql.conf",
    "filename:mongod.conf",
    "filename:redis.conf",
    "filename:cassandra.yaml",
    "filename:elasticsearch.yml"
]

# Save folder in Google Drive
save_dir = "/content/drive/MyDrive/MisconfAI/config_collection/databases"
os.makedirs(save_dir, exist_ok=True)

# Check if text is mostly English
def is_english(text):
    try:
        ascii_text = text.encode('ascii')
        return True
    except UnicodeEncodeError:
        return False

# Fetch configuration files from GitHub
def fetch_files():
    for query in queries:
        print(f" Searching for: {query}")
        for page in range(1, 2):
            url = f"https://api.github.com/search/code?q={query}&per_page=100&page={page}"
            response = requests.get(url, headers=headers).json()
            items = response.get('items', [])

            if not items:
                break

            for item in items:
                repo_url = item['html_url'].replace('blob/', '').split('/')
                raw_url = f"https://raw.githubusercontent.com/{repo_url[3]}/{repo_url[4]}/main/{'/'.join(repo_url[6:])}"

                try:
                    file_data = requests.get(raw_url).text
                    if not is_english(file_data) or len(file_data.strip()) < 10:
                        continue


                    filename = "_".join(repo_url[3:]).replace("/", "_")
                    with open(os.path.join(save_dir, filename), 'w', encoding='utf-8') as f:
                        f.write(file_data)
                    print(f" Saved: {filename}")

                except Exception as e:
                    print(f" Error downloading {raw_url}: {e}")
                    continue

fetch_files()
print("\n Configuration file collection completed.")

"""Cloud"""

import requests
import os
from google.colab import drive

# GitHub API Token
GITHUB_TOKEN = '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'
headers = {'Authorization': f'token {GITHUB_TOKEN}'}

# search query for common Cloud Infrastructure
queries = [
    "filename:main.tf",
    "filename:template.yaml",
    "filename:deployment.yaml",
    "filename:ansible.cfg",
    "filename:variables.tf"
]

# Save folder in Google Drive
save_dir = "/content/drive/MyDrive/MisconfAI/config_collection/cloud_infrastructure"
os.makedirs(save_dir, exist_ok=True)

# Check for english
def is_english(text):
    try:
        ascii_text = text.encode('ascii')
        return True
    except UnicodeEncodeError:
        return False

# Fetch configuration files from GitHub
def fetch_files():
    for query in queries:
        print(f" Searching for: {query}")
        for page in range(1, 21):
            url = f"https://api.github.com/search/code?q={query}&per_page=100&page={page}"
            response = requests.get(url, headers=headers).json()
            items = response.get('items', [])

            if not items:
                break

            for item in items:
                repo_url = item['html_url'].replace('blob/', '').split('/')
                raw_url = f"https://raw.githubusercontent.com/{repo_url[3]}/{repo_url[4]}/main/{'/'.join(repo_url[6:])}"

                try:
                    file_data = requests.get(raw_url).text
                    if not is_english(file_data) or len(file_data.strip()) < 10:
                        continue


                    filename = "_".join(repo_url[3:]).replace("/", "_")
                    with open(os.path.join(save_dir, filename), 'w', encoding='utf-8') as f:
                        f.write(file_data)
                    print(f" Saved: {filename}")

                except Exception as e:
                    print(f" Error downloading {raw_url}: {e}")
                    continue

fetch_files()
print("\n Configuration file collection completed.")

"""DevOps"""

import requests
import os
from google.colab import drive

# GitHub API Token
GITHUB_TOKEN = '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'
headers = {'Authorization': f'token {GITHUB_TOKEN}'}

# DevOps-specific configuration files and scripts
queries = [
    "filename:.gitlab-ci.yml",
    "filename:.travis.yml",
    "filename:.github/workflows",
    "filename:docker-compose.yml",
    "filename:Dockerfile",
    "filename:buildspec.yml",
    "filename:jenkinsfile",
    "filename:.circleci/config.yml"
]

# Save directory
save_dir = "/content/drive/MyDrive/MisconfAI/config_collection/devops"
os.makedirs(save_dir, exist_ok=True)

# Check for English
def is_english(text):
    try:
        text.encode('ascii')
        return True
    except UnicodeEncodeError:
        return False

# Function to download and save files
def fetch_files():
    for query in queries:
        print(f"\n🔍 Searching for: {query}")
        for page in range(1, 21):
            url = f"https://api.github.com/search/code?q={query}&per_page=100&page={page}"
            response = requests.get(url, headers=headers).json()
            items = response.get('items', [])

            if not items:
                break

            for item in items:
                repo_url = item['html_url'].replace('blob/', '').split('/')
                raw_url = f"https://raw.githubusercontent.com/{repo_url[3]}/{repo_url[4]}/main/{'/'.join(repo_url[6:])}"

                try:
                    file_data = requests.get(raw_url).text
                    if not is_english(file_data) or len(file_data.strip()) < 10:
                        continue

                    filename = "_".join(repo_url[3:]).replace("/", "_")
                    with open(os.path.join(save_dir, filename), 'w', encoding='utf-8') as f:
                        f.write(file_data)
                    print(f" Saved: {filename}")

                except Exception as e:
                    print(f" Error fetching {raw_url}: {e}")
                    continue

fetch_files()
print("\n DevOps configuration file collection completed.")

"""**Copying Files**"""

import shutil
import os


source_folder = '/content/drive/MyDrive/MisconfAI/config_collection/app_collection'
destination_folder = '/content/drive/MyDrive/MisconfAI/config_collection/application'


os.makedirs(destination_folder, exist_ok=True)

# Copy files
for filename in os.listdir(source_folder):
    source_path = os.path.join(source_folder, filename)
    destination_path = os.path.join(destination_folder, filename)

    if os.path.isfile(source_path):
        shutil.copy(source_path, destination_path)
        print(f"Copied: {filename}")

print(" All files copied successfully.")

"""**Removing Duplicate Files**"""

import os
import hashlib

def compute_file_hash(filepath):

    hasher = hashlib.sha256()
    with open(filepath, 'rb') as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()

def remove_duplicates(folder_path):
    print(f" Scanning for duplicates in: {folder_path}")
    hashes = {}
    duplicates = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            filepath = os.path.join(root, file)
            try:
                file_hash = compute_file_hash(filepath)
                if file_hash in hashes:
                    print(f" Duplicate found: {filepath} == {hashes[file_hash]}")
                    os.remove(filepath)
                    duplicates.append(filepath)
                else:
                    hashes[file_hash] = filepath
            except Exception as e:
                print(f" Error processing {filepath}: {e}")

    print(f"\n Done. Removed {len(duplicates)} duplicate files.")
    return duplicates


if __name__ == "__main__":
    folder_to_clean = "/content/drive/MyDrive/MisconfAI/config_collection/web_servers"
    remove_duplicates(folder_to_clean)

import os

folder_path = "/content/drive/MyDrive/MisconfAI/config_collection/web_servers"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

import os
import hashlib

def compute_file_hash(filepath):

    hasher = hashlib.sha256()
    with open(filepath, 'rb') as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()

def remove_duplicates(folder_path):
    print(f" Scanning for duplicates in: {folder_path}")
    hashes = {}
    duplicates = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            filepath = os.path.join(root, file)
            try:
                file_hash = compute_file_hash(filepath)
                if file_hash in hashes:
                    print(f" Duplicate found: {filepath} == {hashes[file_hash]}")
                    os.remove(filepath)
                    duplicates.append(filepath)
                else:
                    hashes[file_hash] = filepath
            except Exception as e:
                print(f" Error processing {filepath}: {e}")

    print(f"\n Done. Removed {len(duplicates)} duplicate files.")
    return duplicates


if __name__ == "__main__":
    folder_to_clean = "/content/drive/MyDrive/MisconfAI/config_collection/application"
    remove_duplicates(folder_to_clean)

import os

folder_path = "/content/drive/MyDrive/MisconfAI/config_collection/application"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

import os
import hashlib

def compute_file_hash(filepath):

    hasher = hashlib.sha256()
    with open(filepath, 'rb') as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()

def remove_duplicates(folder_path):
    print(f" Scanning for duplicates in: {folder_path}")
    hashes = {}
    duplicates = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            filepath = os.path.join(root, file)
            try:
                file_hash = compute_file_hash(filepath)
                if file_hash in hashes:
                    print(f" Duplicate found: {filepath} == {hashes[file_hash]}")
                    os.remove(filepath)
                    duplicates.append(filepath)
                else:
                    hashes[file_hash] = filepath
            except Exception as e:
                print(f" Error processing {filepath}: {e}")

    print(f"\n Done. Removed {len(duplicates)} duplicate files.")
    return duplicates

# Example usage
if __name__ == "__main__":
    folder_to_clean = "/content/drive/MyDrive/MisconfAI/config_collection/databases"
    remove_duplicates(folder_to_clean)

import os

folder_path = "/content/drive/MyDrive/MisconfAI/config_collection/databases"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

import os
import hashlib

def compute_file_hash(filepath):
    hasher = hashlib.sha256()
    with open(filepath, 'rb') as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()

def remove_duplicates(folder_path):
    print(f" Scanning for duplicates in: {folder_path}")
    hashes = {}
    duplicates = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            filepath = os.path.join(root, file)
            try:
                file_hash = compute_file_hash(filepath)
                if file_hash in hashes:
                    print(f" Duplicate found: {filepath} == {hashes[file_hash]}")
                    os.remove(filepath)
                    duplicates.append(filepath)
                else:
                    hashes[file_hash] = filepath
            except Exception as e:
                print(f" Error processing {filepath}: {e}")

    print(f"\n Done. Removed {len(duplicates)} duplicate files.")
    return duplicates


if __name__ == "__main__":
    folder_to_clean = "/content/drive/MyDrive/MisconfAI/config_collection/cloud_infrastructure"
    remove_duplicates(folder_to_clean)

import os

folder_path = "/content/drive/MyDrive/MisconfAI/config_collection/cloud_infrastructure"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

import os
import hashlib

def compute_file_hash(filepath):
    hasher = hashlib.sha256()
    with open(filepath, 'rb') as f:
        buf = f.read()
        hasher.update(buf)
    return hasher.hexdigest()

def remove_duplicates(folder_path):
    print(f" Scanning for duplicates in: {folder_path}")
    hashes = {}
    duplicates = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            filepath = os.path.join(root, file)
            try:
                file_hash = compute_file_hash(filepath)
                if file_hash in hashes:
                    print(f" Duplicate found: {filepath} == {hashes[file_hash]}")
                    os.remove(filepath)
                    duplicates.append(filepath)
                else:
                    hashes[file_hash] = filepath
            except Exception as e:
                print(f" Error processing {filepath}: {e}")

    print(f"\n Done. Removed {len(duplicates)} duplicate files.")
    return duplicates


if __name__ == "__main__":
    folder_to_clean = "/content/drive/MyDrive/MisconfAI/config_collection/devops"
    remove_duplicates(folder_to_clean)

import os

folder_path = "/content/drive/MyDrive/MisconfAI/config_collection/devops"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

"""**Preprocessing**"""

import os
import re
import yaml
import json

CATEGORIES = ['web_servers', 'application', 'cloud_infrastructure', 'databases','devops']
INPUT_BASE = '/content/drive/MyDrive/MisconfAI/config_collection'
OUTPUT_BASE = '/content/drive/MyDrive/MisconfAI/preprocessed_configs'


VALID_EXTENSIONS = ('.conf', '.cfg', '.ini', '.yaml', '.yml', '.json', '.properties')

def remove_comments_and_whitespace(content, file_type):
    lines = content.splitlines()
    cleaned = []
    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        # Remove comments
        if file_type in ['.conf', '.cfg', '.ini', '.properties'] and stripped.startswith(('#', ';')):
            continue
        if file_type in ['.yaml', '.yml', '.json'] and stripped.startswith('#'):
            continue
        if '//' in stripped:
            stripped = stripped.split('//')[0].strip()
        cleaned.append(stripped)
    return "\n".join(cleaned)

def normalize_to_dict(content, file_path):

    ext = os.path.splitext(file_path)[1].lower()
    try:
        if ext in ['.yaml', '.yml']:
            return yaml.safe_load(content)
        elif ext == '.json':
            return json.loads(content)
        else:

            parsed = {}
            for line in content.splitlines():
                if '=' in line:
                    key, value = line.split('=', 1)
                    parsed[key.strip()] = value.strip()
            return parsed
    except Exception as e:
        print(f" Skipping {file_path} due to parsing error: {e}")
        return None

def preprocess_category(category):
    input_path = os.path.join(INPUT_BASE, category)
    output_path = os.path.join(OUTPUT_BASE, category)
    os.makedirs(output_path, exist_ok=True)

    print(f" Preprocessing files in '{input_path}' ...")
    count = 0

    for filename in os.listdir(input_path):
        filepath = os.path.join(input_path, filename)
        if not os.path.isfile(filepath):
            continue

        ext = os.path.splitext(filename)[1].lower()
        if ext not in VALID_EXTENSIONS:
            continue

        with open(filepath, 'r', errors='ignore') as f:
            content = f.read()

        cleaned_content = remove_comments_and_whitespace(content, ext)
        normalized_data = normalize_to_dict(cleaned_content, filepath)

        if normalized_data is not None:
            output_filename = os.path.splitext(filename)[0] + '.yaml'
            with open(os.path.join(output_path, output_filename), 'w') as out_f:
                yaml.dump(normalized_data, out_f, default_flow_style=False)
            count += 1

    print(f" {count} files preprocessed and saved in '{output_path}'")

def run_preprocessing():
    for category in CATEGORIES:
        preprocess_category(category)

if __name__ == '__main__':
    run_preprocessing()

import os

folder_path = "/content/drive/MyDrive/MisconfAI/preprocessed_configs/cloud_infrastructure"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

import os

folder_path = "/content/drive/MyDrive/MisconfAI/preprocessed_configs/databases"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

import os

folder_path = "/content/drive/MyDrive/MisconfAI/preprocessed_configs/application"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

import os

folder_path = "/content/drive/MyDrive/MisconfAI/preprocessed_configs/web_servers"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

import os

folder_path = "/content/drive/MyDrive/MisconfAI/preprocessed_configs/devops"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

"""**Augmented learning**"""

import os
import yaml
import random
from pathlib import Path

#Step 1: rules
CWE_EXAMPLES = {
    "CWE-327": ["SSLv3", "TLSv1.0", "TLSv1.1"],
    "CWE-538": ["+Indexes", "on"],
    "CWE-209": ["On", "E_ALL", "true"],
    "CWE-20": ["$input", "%param", "{user}"],
    "CWE-601": ["http://malicious.com", "https://redirect.bad"],
    "CWE-312": ["hardcoded123", "plainsecret", "token123"],
    "CWE-489": ["true"],
    "CWE-942": ["*", "*example.com"],
    "CWE-525": ["false", "off"],
    "CWE-330": ["123", "changeme", "default"],
    "CWE-284": ["0.0.0.0/0", "public", "*"],
    "CWE-200": ["false", "disabled"],
    "CWE-798": ["AKIA123", "ghp_ABC123", "ASIA321"],
    "CWE-16": ["true"],
    "CWE-287": ["1", "true"],
    "CWE-326": ["RC4", "MD5", "DES"],
    "CWE-778": ["off", "false", "disabled"],
    "CWE-93": ["false", "none"],
    "CWE-250": ["true"],
    "CWE-78": ["curl http://evil", "wget http://bad"],
    "CWE-306": ["none", "public"],
    "CWE-611": ["true"]
}

# Step 2: Load all rules from YAML
def load_rules(rules_file):
    with open(rules_file, 'r') as f:
        return yaml.safe_load(f)['rules']

# Step 3: Inject vulnerabilities into a config file
def inject_vulnerabilities(content, rules, max_vulns=5):
    injected = []
    selected_rules = random.sample(rules, k=random.randint(1, min(max_vulns, len(rules))))
    for rule in selected_rules:
        cwe = rule.get("cwe")
        for key in rule.get("keys", []):
            vuln_value = random.choice(CWE_EXAMPLES.get(cwe, ["true"]))
            if isinstance(content, dict):
                content[key] = vuln_value
                injected.append(rule["id"])
    return content, injected

# Step 4: Process all files in a directory for one category
def process_category(input_dir, output_dir, rules, category, metadata, inject_ratio=(0.4, 0.6)):
    files = list(Path(input_dir).glob("*.yaml"))
    num_files = len(files)
    num_vuln = random.randint(int(num_files * inject_ratio[0]), int(num_files * inject_ratio[1]))
    vulnerable_files = set(random.sample(files, k=num_vuln))

    os.makedirs(output_dir, exist_ok=True)

    for file in files:
        with open(file, "r") as f:
            try:
                content = yaml.safe_load(f) or {}
            except:
                continue

        is_vuln = file in vulnerable_files
        injected_rules = []

        if is_vuln:
            content, injected_rules = inject_vulnerabilities(content, rules)

        out_path = Path(output_dir) / file.name
        with open(out_path, "w") as outf:
            yaml.dump(content, outf)

        metadata[file.name] = {
            "category": category,
            "label": "vulnerable" if is_vuln else "non-vulnerable",
            "injected_rules": injected_rules
        }

    return metadata

# Step 5: Run augmentation on all 5 categories
def augment_configs(pre_dir, out_dir, rules_path):
    rules = load_rules(rules_path)
    metadata = {}

    folder_category_map = {
        "application": "APP",
        "webserver": "WS",
        "cloud": "CLOUD",
        "database": "DB",
        "devops": "DEVOPS"
    }

    for folder_name, rule_prefix in folder_category_map.items():
        input_path = os.path.join(pre_dir, folder_name)
        output_path = os.path.join(out_dir, folder_name)
        rule_set = [r for r in rules if r["id"].startswith(rule_prefix)]
        metadata = process_category(input_path, output_path, rule_set, folder_name, metadata)

    with open(os.path.join(out_dir, "metadata.yaml"), "w") as f:
        yaml.dump(metadata, f)

    print(" Augmentation complete! Metadata saved.")

# Example usage
augment_configs(
    pre_dir="/content/drive/MyDrive/MisconfAI/preprocessed_configs",
    out_dir="/content/drive/MyDrive/MisconfAI/augmented_configs",
    rules_path="/content/drive/MyDrive/MisconfAI/rules.yaml"
)

import os

folder_path = "/content/drive/MyDrive/MisconfAI/augmented_configs/devops"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

import os

folder_path = "/content/drive/MyDrive/MisconfAI/augmented_configs/application"

file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])
print(f"Total files in {folder_path}: {file_count}")

"""**Rule Base Analysis**"""

import os
import yaml
import re
import matplotlib.pyplot as plt
from collections import defaultdict

# Load rules from YAML
def load_rules(rules_path):
    with open(rules_path, "r") as f:
        return yaml.safe_load(f)['rules']

# Load metadata from augmentation
def load_metadata(metadata_path):
    with open(metadata_path, 'r') as f:
        return yaml.safe_load(f)

# Match content against rule
def match_rule(content, rule):
    matched_keys = []
    for key in rule.get("keys", []):
        if key in content:
            value = str(content[key])
            if re.search(rule["value_regex"], value):
                matched_keys.append(key)
    return matched_keys

def rule_based_analysis(
    rules_path="/content/drive/MyDrive/MisconfAI/rules.yaml",
    metadata_path="/content/drive/MyDrive/MisconfAI/augmented_configs/metadata.yaml",
    base_path="/content/drive/MyDrive/MisconfAI/augmented_configs"
):
    import matplotlib.pyplot as plt
    import yaml, os, re
    from collections import defaultdict

    rules = load_rules(rules_path)
    metadata = load_metadata(metadata_path)

    result_yaml = {}
    cve_count = defaultdict(int)
    category_summary = defaultdict(lambda: {"vulnerable": 0, "non-vulnerable": 0})

    folder_category_map = {
        "application": "APP",
        "webserver": "WS",
        "cloud": "CLOUD",
        "database": "DB",
        "devops": "DEVOPS"
    }

    for category in folder_category_map:
        cat_path = os.path.join(base_path, category)
        rule_set = [r for r in rules if r["id"].startswith(folder_category_map[category])]

        for file_name in os.listdir(cat_path):
            file_path = os.path.join(cat_path, file_name)

            try:
                with open(file_path, 'r') as f:
                    content = yaml.safe_load(f) or {}
            except:
                continue

            # Label from metadata
            meta = metadata.get(file_name, {})
            label = meta.get("label", "non-vulnerable")
            matched_rules = []

            for rule in rule_set:
                matched_keys = match_rule(content, rule)
                if matched_keys:
                    matched_rules.append({
                        "rule_id": rule["id"],
                        "cwe": rule["cwe"],
                        "description": rule["description"],
                        "severity": rule["severity"],
                        "matched_keys": matched_keys
                    })
                    cve_count[rule["cwe"]] += 1


            if matched_rules:
                label = "vulnerable"

            result_yaml[file_name] = {
                "category": category,
                "label": label,
                "matched_rules": matched_rules
            }

            category_summary[category][label] += 1

    # Save YAML result
    with open("/content/drive/MyDrive/MisconfAI/rule_based_results.yaml", "w") as f:
        yaml.dump(result_yaml, f)

    # Save text summary
    with open("/content/drive/MyDrive/MisconfAI/rule_analysis_results.txt", "w") as f:
        for cat, stats in category_summary.items():
            f.write(f"[{cat.upper()}] - Vulnerable: {stats['vulnerable']} | Non-Vulnerable: {stats['non-vulnerable']}\n")
        f.write("\nCVE Match Count:\n")
        for cwe, count in cve_count.items():
            f.write(f"{cwe}: {count}\n")

    # Save CSV count
    with open("/content/drive/MyDrive/MisconfAI/cve_counts.csv", "w") as f:
        f.write("CWE,Count\n")
        for cwe, count in cve_count.items():
            f.write(f"{cwe},{count}\n")

    # PIE CHARTS for each category
    for cat in category_summary:
        labels = ['vulnerable', 'non-vulnerable']
        values = [
            category_summary[cat].get('vulnerable', 0),
            category_summary[cat].get('non-vulnerable', 0)
        ]
        plt.figure(figsize=(4, 4))
        plt.pie(values, labels=labels, autopct='%1.1f%%', colors=['red', 'green'])
        plt.title(f"{cat.capitalize()} Files - Vulnerability Distribution")
        plt.savefig(f"{cat}_pie_chart.png")
        plt.close()

    # BAR CHART of CWE count
    plt.figure(figsize=(10, 6))
    plt.bar(cve_count.keys(), cve_count.values(), color='purple')
    plt.xticks(rotation=45, ha="right")
    plt.title("CVE Frequency Across All Categories")
    plt.xlabel("CWE ID")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig("cve_bar_chart.png")
    plt.close()

    print("✅ Rule-based analysis completed.")
    print("- YAML: rule_based_results.yaml")
    print("- Summary: rule_analysis_results.txt")
    print("- CVE Count CSV: cve_counts.csv")
    print("- Pie Charts & Bar Chart saved.")

rule_based_analysis()

"""**Run from here now :)**

**From Preprocessed dataset,**
**Have the data about the files already having vulnerability**
"""

import os, yaml, re
import matplotlib.pyplot as plt
from collections import defaultdict
import pandas as pd


RULES_PATH = '/content/drive/MyDrive/MisconfAI/rules.yaml'
INPUT_DIR = '/content/drive/MyDrive/MisconfAI/preprocessed_configs'
RESULT_YAML = '/content/drive/MyDrive/MisconfAI/original_results.yaml'
RESULT_TXT = '/content/drive/MyDrive/MisconfAI/original_analysis_results.txt'
CVE_CSV = '/content/drive/MyDrive/MisconfAI/original_cve_counts.csv'

# Load rules
def load_rules(path):
    with open(path) as f:
        return yaml.safe_load(f)['rules']

# Match rule to file
def match_rule(content, rule):
    matched = []
    for key in rule.get('keys', []):
        if key in content:
            value = str(content[key])
            if re.search(rule.get('value_regex', '.*'), value):
                matched.append({
                    "rule_id": rule["id"],
                    "cwe": rule["cwe"],
                    "description": rule["description"],
                    "severity": rule["severity"],
                    "matched_key": key,
                    "matched_value": value
                })
    return matched

# Rule-based analysis
def rule_based_analysis():
    rules = load_rules(RULES_PATH)
    results = {}
    summary = defaultdict(lambda: {"vulnerable": 0, "non-vulnerable": 0})
    cve_counter = defaultdict(int)

    for category in os.listdir(INPUT_DIR):
        cat_path = os.path.join(INPUT_DIR, category)
        if not os.path.isdir(cat_path):
            continue

        for file in os.listdir(cat_path):
            if not file.endswith('.yaml'):
                continue
            file_path = os.path.join(cat_path, file)
            with open(file_path) as f:
                try:
                    content = yaml.safe_load(f) or {}
                except:
                    continue

            matched_rules = []
            for rule in rules:
                if rule['id'].lower().startswith(category[:3]):
                    matched = match_rule(content, rule)
                    matched_rules.extend(matched)
                    for m in matched:
                        cve_counter[m["cwe"]] += 1

            label = 'vulnerable' if matched_rules else 'non-vulnerable'
            results[file] = {
                "category": category,
                "label": label,
                "matched_rules": matched_rules
            }
            summary[category][label] += 1

    # Save YAML results
    with open(RESULT_YAML, 'w') as f:
        yaml.dump(results, f)

    # Save summary text
    with open(RESULT_TXT, 'w') as f:
        for cat, stats in summary.items():
            f.write(f"{cat} - Vulnerable: {stats['vulnerable']}, Non-Vulnerable: {stats['non-vulnerable']}\n")

    # Save CVE CSV
    pd.DataFrame(list(cve_counter.items()), columns=["CWE", "Count"]).to_csv(CVE_CSV, index=False)

    print("✅ Rule-based analysis complete. Outputs saved.")

    # Plot pie charts
    for cat, stat in summary.items():
        labels = ['Vulnerable', 'Non-Vulnerable']
        values = [stat['vulnerable'], stat['non-vulnerable']]
        plt.figure(figsize=(5, 5))
        plt.pie(values, labels=labels, autopct='%1.1f%%', colors=['red', 'green'])
        plt.title(f"{cat.title()} - Vulnerability Distribution")
        plt.savefig(f"/content/drive/MyDrive/MisconfAI/pie_{cat}.png")
        plt.close()

    # Bar chart for CWE frequencies
    if cve_counter:
        sorted_cve = dict(sorted(cve_counter.items(), key=lambda item: item[1], reverse=True))
        plt.figure(figsize=(12, 6))
        plt.bar(sorted_cve.keys(), sorted_cve.values(), color='skyblue')
        plt.xticks(rotation=45)
        plt.title("CVE Rule Match Frequency")
        plt.xlabel("CWE ID")
        plt.ylabel("Matches")
        plt.tight_layout()
        plt.savefig("/content/drive/MyDrive/MisconfAI/cve_bar_chart.png")
        plt.close()
        print("📊 Visualizations saved.")


rule_based_analysis()

"""**Augmentation ML based**"""

import os
import yaml
import random
from pathlib import Path
from collections import defaultdict

# CWE examples
CWE_EXAMPLES = {
    "CWE-327": ["SSLv3", "Using legacy SSL settings", "Obsolete TLSv1.0"],
    "CWE-538": ["+Indexes", "autoindex on", "directory listing enabled"],
    "CWE-209": ["E_ALL", "verbose", "show_errors=true"],
    "CWE-20": ["userinput=${data}", "input: %param%", "param: {user}"],
    "CWE-601": ["http://example.com/redirect", "https://bad.site/return"],
    "CWE-312": ["password=123456", "token=hardcoded_value"],
    "CWE-489": ["debug: true", "enableDebug: yes"],
    "CWE-942": ["cors=*", "allowed_origins: '*'"],
    "CWE-614": ["SESSION_COOKIE_SECURE: false", "COOKIE_HTTPONLY: off"],
    "CWE-330": ["SECRET_KEY=changeme", "defaultkey=123"],
    "CWE-284": ["cidr_blocks: 0.0.0.0/0", "public_access=true"],
    "CWE-200": ["encryption: false"],
    "CWE-798": ["access_key=AKIA12345", "secret_key=ghp_token"],
    "CWE-16": ["default=true"],
    "CWE-287": ["skip-grant-tables=1"],
    "CWE-326": ["ssl_cipher=RC4"],
    "CWE-778": ["audit_log=off"],
    "CWE-93": ["sanitize_inputs: false"],
    "CWE-250": ["privileged=true"],
    "CWE-78": ["script: curl http://evil.com"],
    "CWE-306": ["access_control: public"],
    "CWE-611": ["externalEntities: true"]
}

CWE_SUBTLE_VARIATIONS = {
    "CWE-327": ["tls: legacy_enabled", "security: deprecated_ssl"],
    "CWE-538": ["features: indexable", "listing: show"],
    "CWE-209": ["error_visibility: full", "reveal_exceptions: true"],
    "CWE-601": ["redirect_to: user_input"],
    "CWE-312": ["key: abc123", "auth: staticToken"]
}

def load_rules(rules_path):
    with open(rules_path) as f:
        return yaml.safe_load(f)["rules"]


def inject_mixed_vulnerabilities(content, rules, max_vulns=3, for_ml=False):
    injected = []
    if not isinstance(content, dict):
        return content, injected

    selected_rules = random.sample(rules, k=random.randint(1, max_vulns))
    for rule in selected_rules:
        cwe = rule.get("cwe")
        keys = rule.get("keys", [])
        value_list = CWE_SUBTLE_VARIATIONS.get(cwe) if for_ml else CWE_EXAMPLES.get(cwe)
        if not value_list:
            continue
        for key in keys:
            content[key] = random.choice(value_list)
        injected.append(rule["id"])
    return content, injected

def process_category(input_dir, output_dir, rules, category, metadata, inject_ratio=(0.4, 0.6)):
    files = list(Path(input_dir).glob("*.yaml"))
    num_files = len(files)
    num_vuln = random.randint(int(num_files * inject_ratio[0]), int(num_files * inject_ratio[1]))
    vuln_files = set(random.sample(files, k=num_vuln))
    os.makedirs(output_dir, exist_ok=True)

    for file in files:
        with open(file, "r") as f:
            try:
                content = yaml.safe_load(f)
                if not isinstance(content, dict):
                    continue
            except:
                continue

        is_vuln = file in vuln_files
        injected_rules = []
        ml_like = random.random() < 0.5

        if is_vuln:
            content, injected_rules = inject_mixed_vulnerabilities(content, rules, for_ml=ml_like)

        with open(Path(output_dir) / file.name, "w") as outf:
            yaml.dump(content, outf)

        metadata[file.name] = {
            "category": category,
            "label": "vulnerable" if is_vuln else "non-vulnerable",
            "injected_rules": injected_rules,
            "type": "ml-like" if is_vuln and ml_like else "rule-like" if is_vuln else "clean"
        }

    return metadata

def augment_all(pre_dir, out_dir, rules_path):
    rules = load_rules(rules_path)
    metadata = {}

    categories = {
        "application": [r for r in rules if r["id"].startswith("APP")],
        "webserver": [r for r in rules if r["id"].startswith("WS")],
        "cloud": [r for r in rules if r["id"].startswith("CLOUD")],
        "database": [r for r in rules if r["id"].startswith("DB")],
        "devops": [r for r in rules if r["id"].startswith("DEVOPS")]
    }

    for cat, rule_set in categories.items():
        input_path = os.path.join(pre_dir, cat)
        output_path = os.path.join(out_dir, cat)
        metadata = process_category(input_path, output_path, rule_set, cat, metadata)

    metadata_path = os.path.join(out_dir, "metadata.yaml")
    with open(metadata_path, "w") as f:
        yaml.dump(metadata, f)

    print(f" Augmentation complete. Metadata saved to: {metadata_path}")
    return metadata_path, metadata

# Run
augment_all(
    pre_dir="/content/drive/MyDrive/MisconfAI/preprocessed_configs",
    out_dir="/content/drive/MyDrive/MisconfAI/ml_augmented_configs",
    rules_path="/content/drive/MyDrive/MisconfAI/rules.yaml"
)

"""**Above one was not generating cwe for ml like**"""

import os
import yaml
import random
from pathlib import Path
from collections import defaultdict

# CWE examples
CWE_EXAMPLES = {
    "CWE-327": ["SSLv3", "Using legacy SSL settings", "Obsolete TLSv1.0"],
    "CWE-538": ["+Indexes", "autoindex on", "directory listing enabled"],
    "CWE-209": ["E_ALL", "verbose", "show_errors=true"],
    "CWE-20": ["userinput=${data}", "input: %param%", "param: {user}"],
    "CWE-601": ["http://example.com/redirect", "https://bad.site/return"],
    "CWE-312": ["password=123456", "token=hardcoded_value"],
    "CWE-489": ["debug: true", "enableDebug: yes"],
    "CWE-942": ["cors=*", "allowed_origins: '*'"],
    "CWE-614": ["SESSION_COOKIE_SECURE: false", "COOKIE_HTTPONLY: off"],
    "CWE-330": ["SECRET_KEY=changeme", "defaultkey=123"],
    "CWE-284": ["cidr_blocks: 0.0.0.0/0", "public_access=true"],
    "CWE-200": ["encryption: false"],
    "CWE-798": ["access_key=AKIA12345", "secret_key=ghp_token"],
    "CWE-16": ["default=true"],
    "CWE-287": ["skip-grant-tables=1"],
    "CWE-326": ["ssl_cipher=RC4"],
    "CWE-778": ["audit_log=off"],
    "CWE-93": ["sanitize_inputs: false"],
    "CWE-250": ["privileged=true"],
    "CWE-78": ["script: curl http://evil.com"],
    "CWE-306": ["access_control: public"],
    "CWE-611": ["externalEntities: true"]
}

CWE_SUBTLE_VARIATIONS = {
    "CWE-327": ["tls: legacy_enabled", "security: deprecated_ssl"],
    "CWE-538": ["features: indexable", "listing: show"],
    "CWE-209": ["error_visibility: full", "reveal_exceptions: true"],
    "CWE-601": ["redirect_to: user_input"],
    "CWE-312": ["key: abc123", "auth: staticToken"]
}

def load_rules(rules_path):
    with open(rules_path) as f:
        return yaml.safe_load(f)["rules"]


def inject_mixed_vulnerabilities(content, rules, max_vulns=3, for_ml=False):
    injected = []
    if not isinstance(content, dict):
        return content, injected

    selected_rules = random.sample(rules, k=random.randint(1, max_vulns))
    for rule in selected_rules:
        cwe = rule.get("cwe")
        rule_id = rule.get("id")
        keys = rule.get("keys", [])
        value_list = CWE_SUBTLE_VARIATIONS.get(cwe) if for_ml else CWE_EXAMPLES.get(cwe)
        if not value_list:
            continue
        for key in keys:
            content[key] = random.choice(value_list)
        injected.append({"id": rule_id, "cwe": cwe})
    return content, injected

def process_category(input_dir, output_dir, rules, category, metadata, inject_ratio=(0.4, 0.6)):
    files = list(Path(input_dir).glob("*.yaml"))
    num_files = len(files)
    num_vuln = random.randint(int(num_files * inject_ratio[0]), int(num_files * inject_ratio[1]))
    vuln_files = set(random.sample(files, k=num_vuln))
    os.makedirs(output_dir, exist_ok=True)

    for file in files:
        with open(file, "r") as f:
            try:
                content = yaml.safe_load(f)
                if not isinstance(content, dict):
                    continue
            except:
                continue

        is_vuln = file in vuln_files
        vulnerable_rules = []
        ml_like = random.random() < 0.5

        if is_vuln:
            content, injected = inject_mixed_vulnerabilities(content, rules, for_ml=ml_like)
            vulnerable_rules = injected

        with open(Path(output_dir) / file.name, "w") as outf:
            yaml.dump(content, outf)

        metadata[file.name] = {
            "category": category,
            "label": "vulnerable" if is_vuln else "non-vulnerable",
            "vulnerable_rules": vulnerable_rules,
            "type": "ml-like" if is_vuln and ml_like else "rule-like" if is_vuln else "clean"
        }

    return metadata

def augment_all(pre_dir, out_dir, rules_path):
    rules = load_rules(rules_path)
    metadata = {}

    categories = {
        "application": [r for r in rules if r["id"].startswith("APP")],
        "webserver": [r for r in rules if r["id"].startswith("WS")],
        "cloud": [r for r in rules if r["id"].startswith("CLOUD")],
        "database": [r for r in rules if r["id"].startswith("DB")],
        "devops": [r for r in rules if r["id"].startswith("DEVOPS")]
    }

    for cat, rule_set in categories.items():
        input_path = os.path.join(pre_dir, cat)
        output_path = os.path.join(out_dir, cat)
        metadata = process_category(input_path, output_path, rule_set, cat, metadata)

    metadata_path = os.path.join(out_dir, "metadata.yaml")
    with open(metadata_path, "w") as f:
        yaml.dump(metadata, f)

    print(f" Augmentation complete. Metadata saved to: {metadata_path}")
    return metadata_path, metadata

# Run the process
augment_all(
    pre_dir="/content/drive/MyDrive/MisconfAI/preprocessed_configs",
    out_dir="/content/drive/MyDrive/MisconfAI/augmented_configs",
    rules_path="/content/drive/MyDrive/MisconfAI/rules.yaml"
)

"""**Combining the augmentation+ orginal metadata**"""

import os
import yaml
import matplotlib.pyplot as plt
from collections import defaultdict, Counter
import pandas as pd


rule_based_metadata_path = '/content/drive/MyDrive/MisconfAI/Original Data/original_results.yaml'
ml_augmented_metadata_path = '/content/drive/MyDrive/MisconfAI/ml_augmented_configs/metadata.yaml'
merged_metadata_path = '/content/drive/MyDrive/MisconfAI/merged_metadata.yaml'

# Load and merge metadata
def load_metadata(path):
    with open(path) as f:
        return yaml.safe_load(f)

def merge_metadata(rule_data, ml_data):
    merged = rule_data.copy()
    for fname, info in ml_data.items():
        if fname in merged:
            merged[fname].update(info)
        else:
            merged[fname] = info
    return merged

# Count CWE frequency by type and category
def analyze_cwes(merged_metadata):
    cwe_by_type = defaultdict(Counter)
    cwe_total = Counter()
    category_counts = defaultdict(lambda: {'vulnerable': 0, 'non-vulnerable': 0})

    for file, meta in merged_metadata.items():
        label = meta.get('label', 'non-vulnerable')
        cat = meta.get('category', 'unknown')
        typ = meta.get('type', 'rule-like' if label == 'vulnerable' else 'clean')

        if label == 'vulnerable':
            category_counts[cat]['vulnerable'] += 1
        else:
            category_counts[cat]['non-vulnerable'] += 1

        for rule in meta.get('injected_rules', []):
            if rule.startswith("APP-"):
                cwe = "APP"
            elif rule.startswith("WS-"):
                cwe = "WS"
            elif rule.startswith("DB-"):
                cwe = "DB"
            elif rule.startswith("CLOUD-"):
                cwe = "CLOUD"
            elif rule.startswith("DEVOPS-"):
                cwe = "DEVOPS"
            else:
                cwe = "Other"

            cwe_total[cwe] += 1
            cwe_by_type[typ][cwe] += 1

    return cwe_by_type, cwe_total, category_counts

# Plotting charts
def plot_pie_and_bar(cwe_by_type, cwe_total, category_counts):
    os.makedirs('/content/drive/MyDrive/MisconfAI/Aug data/charts', exist_ok=True)

    for typ, cwes in cwe_by_type.items():
        plt.figure(figsize=(6, 6))
        plt.title(f'CWE Distribution - {typ}')
        plt.pie(cwes.values(), labels=cwes.keys(), autopct='%1.1f%%')
        plt.tight_layout()
        plt.savefig(f'/content/drive/MyDrive/MisconfAI/Aug data/charts/cwe_pie_{typ}.png')
        plt.close()

    # Bar chart for overall CWE frequency
    plt.figure(figsize=(8, 5))
    plt.title('CWE Frequency Overall')
    plt.bar(cwe_total.keys(), cwe_total.values(), color='skyblue')
    plt.ylabel('Frequency')
    plt.xlabel('CWE Group')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/MisconfAI/Aug data/charts/cwe_bar_overall.png')
    plt.close()

    # Bar chart for category-wise vulnerability count
    cats = list(category_counts.keys())
    vuln = [category_counts[c]['vulnerable'] for c in cats]
    non_vuln = [category_counts[c]['non-vulnerable'] for c in cats]

    plt.figure(figsize=(8, 5))
    x = range(len(cats))
    plt.bar(x, vuln, width=0.4, label='Vulnerable', color='red', align='center')
    plt.bar(x, non_vuln, width=0.4, label='Non-Vulnerable', color='green', align='edge')
    plt.xticks(x, cats, rotation=45)
    plt.title('Vulnerable vs Non-Vulnerable Files per Category')
    plt.ylabel('Number of Files')
    plt.legend()
    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/MisconfAI/Aug data/charts/category_vuln_distribution.png')
    plt.close()

# Run
rule_metadata = load_metadata(rule_based_metadata_path)
ml_metadata = load_metadata(ml_augmented_metadata_path)
merged_metadata = merge_metadata(rule_metadata, ml_metadata)

with open(merged_metadata_path, 'w') as f:
    yaml.dump(merged_metadata, f)

cwe_by_type, cwe_total, category_counts = analyze_cwes(merged_metadata)
plot_pie_and_bar(cwe_by_type, cwe_total, category_counts)

{
    "status": " Done",
    "merged_metadata": merged_metadata_path,
    "charts_saved_to": "/content/drive/MyDrive/MisconfAI/charts"
}

import os
import yaml
import matplotlib.pyplot as plt
from collections import defaultdict, Counter
import pandas as pd


rule_based_metadata_path = '/content/drive/MyDrive/MisconfAI/Original Data/original_results.yaml'
ml_augmented_metadata_path = '/content/drive/MyDrive/MisconfAI/augmented_configs/metadata.yaml'
merged_metadata_path = '/content/drive/MyDrive/MisconfAI/metadata.yaml'

# Load and merge metadata
def load_metadata(path):
    with open(path) as f:
        return yaml.safe_load(f)

def merge_metadata(rule_data, ml_data):
    merged = rule_data.copy()
    for fname, info in ml_data.items():
        if fname in merged:
            merged[fname].update(info)
        else:
            merged[fname] = info
    return merged

# Count CWE frequency by type and category
def analyze_cwes(merged_metadata):
    cwe_by_type = defaultdict(Counter)
    cwe_total = Counter()
    category_counts = defaultdict(lambda: {'vulnerable': 0, 'non-vulnerable': 0})

    for file, meta in merged_metadata.items():
        label = meta.get('label', 'non-vulnerable')
        cat = meta.get('category', 'unknown')
        typ = meta.get('type', 'rule-like' if label == 'vulnerable' else 'clean')

        if label == 'vulnerable':
            category_counts[cat]['vulnerable'] += 1
        else:
            category_counts[cat]['non-vulnerable'] += 1

        for rule in meta.get('injected_rules', []):
            if rule.startswith("APP-"):
                cwe = "APP"
            elif rule.startswith("WS-"):
                cwe = "WS"
            elif rule.startswith("DB-"):
                cwe = "DB"
            elif rule.startswith("CLOUD-"):
                cwe = "CLOUD"
            elif rule.startswith("DEVOPS-"):
                cwe = "DEVOPS"
            else:
                cwe = "Other"

            cwe_total[cwe] += 1
            cwe_by_type[typ][cwe] += 1

    return cwe_by_type, cwe_total, category_counts

# Plotting charts
def plot_pie_and_bar(cwe_by_type, cwe_total, category_counts):
    os.makedirs('/content/drive/MyDrive/MisconfAI/augmented_data/charts', exist_ok=True)

    for typ, cwes in cwe_by_type.items():
        plt.figure(figsize=(6, 6))
        plt.title(f'CWE Distribution - {typ}')
        plt.pie(cwes.values(), labels=cwes.keys(), autopct='%1.1f%%')
        plt.tight_layout()
        plt.savefig(f'/content/drive/MyDrive/MisconfAI/augmented_data/charts/cwe_pie_{typ}.png')
        plt.close()

    # Bar chart for overall CWE frequency
    plt.figure(figsize=(8, 5))
    plt.title('CWE Frequency Overall')
    plt.bar(cwe_total.keys(), cwe_total.values(), color='skyblue')
    plt.ylabel('Frequency')
    plt.xlabel('CWE Group')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/MisconfAI/augmented_data/charts/cwe_bar_overall.png')
    plt.close()

    # Bar chart for category-wise vulnerability count
    cats = list(category_counts.keys())
    vuln = [category_counts[c]['vulnerable'] for c in cats]
    non_vuln = [category_counts[c]['non-vulnerable'] for c in cats]

    plt.figure(figsize=(8, 5))
    x = range(len(cats))
    plt.bar(x, vuln, width=0.4, label='Vulnerable', color='red', align='center')
    plt.bar(x, non_vuln, width=0.4, label='Non-Vulnerable', color='green', align='edge')
    plt.xticks(x, cats, rotation=45)
    plt.title('Vulnerable vs Non-Vulnerable Files per Category')
    plt.ylabel('Number of Files')
    plt.legend()
    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/MisconfAI/augmented_data/charts/category_vuln_distribution.png')
    plt.close()

# Run pipeline
rule_metadata = load_metadata(rule_based_metadata_path)
ml_metadata = load_metadata(ml_augmented_metadata_path)
merged_metadata = merge_metadata(rule_metadata, ml_metadata)

with open(merged_metadata_path, 'w') as f:
    yaml.dump(merged_metadata, f)

cwe_by_type, cwe_total, category_counts = analyze_cwes(merged_metadata)
plot_pie_and_bar(cwe_by_type, cwe_total, category_counts)

{
    "status": " Done",
    "merged_metadata": merged_metadata_path,
    "charts_saved_to": "/content/drive/MyDrive/MisconfAI/charts"
}

"""**Detail about the augmented data**

**Category wise distribution**
"""

import yaml
from collections import defaultdict

with open("/content/drive/MyDrive/MisconfAI/augmented_configs/metadata.yaml", "r") as f:
    data = yaml.safe_load(f)

vuln_per_category = defaultdict(int)

for file_info in data.values():
    if file_info["label"] == "vulnerable":
        vuln_per_category[file_info["category"]] += 1

print("Vulnerable files per category:")
for category, count in vuln_per_category.items():
    print(f"{category}: {count}")

"""**Different type**"""

from collections import defaultdict

rule_vs_ml_per_category = defaultdict(lambda: {"rule-like": 0, "ml-like": 0})

for file_info in data.values():
    if file_info["label"] == "vulnerable":
        category = file_info["category"]
        vtype = file_info["type"]
        rule_vs_ml_per_category[category][vtype] += 1

print("Rule-like vs ML-like vulnerable files per category:")
for category, counts in rule_vs_ml_per_category.items():
    print(f"{category}: rule-like={counts['rule-like']}, ml-like={counts['ml-like']}")

"""**Frequency Over CWE**"""

from collections import Counter

cwe_counter = Counter()

for file_info in data.values():
    if file_info["label"] == "vulnerable":
        for rule in file_info.get("vulnerable_rules", []):
            cwe = rule.get("cwe")
            if cwe:
                cwe_counter[cwe] += 1

print("CWE frequency:")
for cwe, count in cwe_counter.items():
    print(f"{cwe}: {count}")

"""**Saving the above results**"""

import yaml
import os
import json
from collections import defaultdict, Counter

# Load metadata
with open("/content/drive/MyDrive/MisconfAI/augmented_configs/metadata.yaml", "r") as f:
    data = yaml.safe_load(f)

# Create output folder
output_dir = "/content/drive/MyDrive/MisconfAI/augmented_data"
os.makedirs(output_dir, exist_ok=True)

# 1. Vulnerable Count per Category
vuln_per_category = defaultdict(int)
for file_info in data.values():
    if file_info["label"] == "vulnerable":
        vuln_per_category[file_info["category"]] += 1

with open(os.path.join(output_dir, "vulnerable_per_category.json"), "w") as f:
    json.dump(vuln_per_category, f, indent=2)

with open(os.path.join(output_dir, "vulnerable_per_category.txt"), "w") as f:
    for category, count in vuln_per_category.items():
        f.write(f"{category}: {count}\n")

# 2. per Category
rule_vs_ml_per_category = defaultdict(lambda: {"rule-like": 0, "ml-like": 0})
for file_info in data.values():
    if file_info["label"] == "vulnerable":
        category = file_info["category"]
        vtype = file_info["type"]
        rule_vs_ml_per_category[category][vtype] += 1

with open(os.path.join(output_dir, "rule_vs_ml_per_category.json"), "w") as f:
    json.dump(rule_vs_ml_per_category, f, indent=2)

with open(os.path.join(output_dir, "rule_vs_ml_per_category.txt"), "w") as f:
    for category, counts in rule_vs_ml_per_category.items():
        f.write(f"{category}: rule-like={counts['rule-like']}, ml-like={counts['ml-like']}\n")

# 3. CWE Frequency
cwe_counter = Counter()
for file_info in data.values():
    if file_info["label"] == "vulnerable":
        for rule in file_info.get("vulnerable_rules", []):
            cwe = rule.get("cwe")
            if cwe:
                cwe_counter[cwe] += 1

with open(os.path.join(output_dir, "cwe_frequency.json"), "w") as f:
    json.dump(dict(cwe_counter), f, indent=2)

with open(os.path.join(output_dir, "cwe_frequency.txt"), "w") as f:
    for cwe, count in cwe_counter.items():
        f.write(f"{cwe}: {count}\n")

print("All analysis results saved in folder.")

"""**Final augmentation as 1 to 5 vulnerability**"""

import os
import yaml
import random
from pathlib import Path
from collections import defaultdict

# CWE examples
CWE_EXAMPLES = {
    "CWE-327": ["SSLv3", "Using legacy SSL settings", "Obsolete TLSv1.0"],
    "CWE-538": ["+Indexes", "autoindex on", "directory listing enabled"],
    "CWE-209": ["E_ALL", "verbose", "show_errors=true"],
    "CWE-20": ["userinput=${data}", "input: %param%", "param: {user}"],
    "CWE-601": ["http://example.com/redirect", "https://bad.site/return"],
    "CWE-312": ["password=123456", "token=hardcoded_value"],
    "CWE-489": ["debug: true", "enableDebug: yes"],
    "CWE-942": ["cors=*", "allowed_origins: '*'"],
    "CWE-614": ["SESSION_COOKIE_SECURE: false", "COOKIE_HTTPONLY: off"],
    "CWE-330": ["SECRET_KEY=changeme", "defaultkey=123"],
    "CWE-284": ["cidr_blocks: 0.0.0.0/0", "public_access=true"],
    "CWE-200": ["encryption: false"],
    "CWE-798": ["access_key=AKIA12345", "secret_key=ghp_token"],
    "CWE-16": ["default=true"],
    "CWE-287": ["skip-grant-tables=1"],
    "CWE-326": ["ssl_cipher=RC4"],
    "CWE-778": ["audit_log=off"],
    "CWE-93": ["sanitize_inputs: false"],
    "CWE-250": ["privileged=true"],
    "CWE-78": ["script: curl http://evil.com"],
    "CWE-306": ["access_control: public"],
    "CWE-611": ["externalEntities: true"]
}

CWE_SUBTLE_VARIATIONS = {
    "CWE-327": ["tls: legacy_enabled", "security: deprecated_ssl"],
    "CWE-538": ["features: indexable", "listing: show"],
    "CWE-209": ["error_visibility: full", "reveal_exceptions: true"],
    "CWE-601": ["redirect_to: user_input"],
    "CWE-312": ["key: abc123", "auth: staticToken"]
}

def load_rules(rules_path):
    with open(rules_path) as f:
        return yaml.safe_load(f)["rules"]


def inject_mixed_vulnerabilities(content, rules, max_vulns=5, for_ml=False):
    injected = []
    if not isinstance(content, dict):
        return content, injected

    selected_rules = random.sample(rules, k=random.randint(1, max_vulns))
    for rule in selected_rules:
        cwe = rule.get("cwe")
        rule_id = rule.get("id")
        keys = rule.get("keys", [])
        value_list = CWE_SUBTLE_VARIATIONS.get(cwe) if for_ml else CWE_EXAMPLES.get(cwe)
        if not value_list:
            continue
        for key in keys:
            content[key] = random.choice(value_list)
        injected.append({"id": rule_id, "cwe": cwe})
    return content, injected

def process_category(input_dir, output_dir, rules, category, metadata, inject_ratio=(0.4, 0.6)):
    files = list(Path(input_dir).glob("*.yaml"))
    num_files = len(files)
    num_vuln = random.randint(int(num_files * inject_ratio[0]), int(num_files * inject_ratio[1]))
    vuln_files = set(random.sample(files, k=num_vuln))
    os.makedirs(output_dir, exist_ok=True)

    for file in files:
        with open(file, "r") as f:
            try:
                content = yaml.safe_load(f)
                if not isinstance(content, dict):
                    continue
            except:
                continue

        is_vuln = file in vuln_files
        vulnerable_rules = []
        ml_like = random.random() < 0.5

        if is_vuln:
            content, injected = inject_mixed_vulnerabilities(content, rules, for_ml=ml_like)
            vulnerable_rules = injected

        with open(Path(output_dir) / file.name, "w") as outf:
            yaml.dump(content, outf)

        metadata[file.name] = {
            "category": category,
            "label": "vulnerable" if is_vuln else "non-vulnerable",
            "vulnerable_rules": vulnerable_rules,
            "type": "ml-like" if is_vuln and ml_like else "rule-like" if is_vuln else "clean"
        }

    return metadata

def augment_all(pre_dir, out_dir, rules_path):
    rules = load_rules(rules_path)
    metadata = {}

    categories = {
        "application": [r for r in rules if r["id"].startswith("APP")],
        "webserver": [r for r in rules if r["id"].startswith("WS")],
        "cloud": [r for r in rules if r["id"].startswith("CLOUD")],
        "database": [r for r in rules if r["id"].startswith("DB")],
        "devops": [r for r in rules if r["id"].startswith("DEVOPS")]
    }

    for cat, rule_set in categories.items():
        input_path = os.path.join(pre_dir, cat)
        output_path = os.path.join(out_dir, cat)
        metadata = process_category(input_path, output_path, rule_set, cat, metadata)

    metadata_path = os.path.join(out_dir, "metadata.yaml")
    with open(metadata_path, "w") as f:
        yaml.dump(metadata, f)

    print(f" Augmentation complete. Metadata saved to: {metadata_path}")
    return metadata_path, metadata

# Run the process
augment_all(
    pre_dir="/content/drive/MyDrive/MisconfAI/preprocessed_configs",
    out_dir="/content/drive/MyDrive/MisconfAI/finalaugmented_configs",
    rules_path="/content/drive/MyDrive/MisconfAI/rules.yaml"
)

"""**Details about the final augmentation**"""

import yaml
from collections import defaultdict

with open("/content/drive/MyDrive/MisconfAI/finalaugmented_configs/metadata.yaml", "r") as f:
    data = yaml.safe_load(f)

vuln_per_category = defaultdict(int)

for file_info in data.values():
    if file_info["label"] == "vulnerable":
        vuln_per_category[file_info["category"]] += 1

print("Vulnerable files per category:")
for category, count in vuln_per_category.items():
    print(f"{category}: {count}")

from collections import defaultdict

rule_vs_ml_per_category = defaultdict(lambda: {"rule-like": 0, "ml-like": 0})

for file_info in data.values():
    if file_info["label"] == "vulnerable":
        category = file_info["category"]
        vtype = file_info["type"]
        rule_vs_ml_per_category[category][vtype] += 1

print("Rule-like vs ML-like vulnerable files per category:")
for category, counts in rule_vs_ml_per_category.items():
    print(f"{category}: rule-like={counts['rule-like']}, ml-like={counts['ml-like']}")

from collections import Counter

cwe_counter = Counter()

for file_info in data.values():
    if file_info["label"] == "vulnerable":
        for rule in file_info.get("vulnerable_rules", []):
            cwe = rule.get("cwe")
            if cwe:
                cwe_counter[cwe] += 1

print("CWE frequency:")
for cwe, count in cwe_counter.items():
    print(f"{cwe}: {count}")

import yaml
import os
import json
from collections import defaultdict, Counter

# Load metadata
with open("/content/drive/MyDrive/MisconfAI/finalaugmented_configs/metadata.yaml", "r") as f:
    data = yaml.safe_load(f)

# Create output folder
output_dir = "/content/drive/MyDrive/MisconfAI/finalaugmented_data"
os.makedirs(output_dir, exist_ok=True)

# 1. Vulnerable Count per Category
vuln_per_category = defaultdict(int)
for file_info in data.values():
    if file_info["label"] == "vulnerable":
        vuln_per_category[file_info["category"]] += 1

with open(os.path.join(output_dir, "vulnerable_per_category.json"), "w") as f:
    json.dump(vuln_per_category, f, indent=2)

with open(os.path.join(output_dir, "vulnerable_per_category.txt"), "w") as f:
    for category, count in vuln_per_category.items():
        f.write(f"{category}: {count}\n")

# 2. Rule-Based vs ML-Like per Category
rule_vs_ml_per_category = defaultdict(lambda: {"rule-like": 0, "ml-like": 0})
for file_info in data.values():
    if file_info["label"] == "vulnerable":
        category = file_info["category"]
        vtype = file_info["type"]
        rule_vs_ml_per_category[category][vtype] += 1

with open(os.path.join(output_dir, "rule_vs_ml_per_category.json"), "w") as f:
    json.dump(rule_vs_ml_per_category, f, indent=2)

with open(os.path.join(output_dir, "rule_vs_ml_per_category.txt"), "w") as f:
    for category, counts in rule_vs_ml_per_category.items():
        f.write(f"{category}: rule-like={counts['rule-like']}, ml-like={counts['ml-like']}\n")

# 3. CWE Frequency
cwe_counter = Counter()
for file_info in data.values():
    if file_info["label"] == "vulnerable":
        for rule in file_info.get("vulnerable_rules", []):
            cwe = rule.get("cwe")
            if cwe:
                cwe_counter[cwe] += 1

with open(os.path.join(output_dir, "cwe_frequency.json"), "w") as f:
    json.dump(dict(cwe_counter), f, indent=2)

with open(os.path.join(output_dir, "cwe_frequency.txt"), "w") as f:
    for cwe, count in cwe_counter.items():
        f.write(f"{cwe}: {count}\n")

print("All analysis results saved in folder.")

"""**Rule Based Analysis**

**Now rule based analysis on augmentation**
"""

import os, yaml, re
import matplotlib.pyplot as plt
from collections import defaultdict
import pandas as pd


RULES_PATH = '/content/drive/MyDrive/MisconfAI/rules.yaml'
INPUT_DIR = '/content/drive/MyDrive/MisconfAI/ml_augmented_configs'
RESULT_YAML = '/content/drive/MyDrive/MisconfAI/rule_results.yaml'
RESULT_TXT = '/content/drive/MyDrive/MisconfAI/rule_analysis_results.txt'
CVE_CSV = '/content/drive/MyDrive/MisconfAI/cve_counts.csv'
BAR_PLOT = '/content/drive/MyDrive/MisconfAI/cwe_bar_chart.png'
PIE_BASE = '/content/drive/MyDrive/MisconfAI/pie_ml_aug_'

#rules load
def load_rules(path):
    with open(path) as f:
        return yaml.safe_load(f)['rules']

# match
def match_rule(content, rule):
    matches = []
    for key in rule.get('keys', []):
        if key in content:
            value = str(content[key])
            if re.search(rule.get('value_regex', '.*'), value):
                matches.append({
                    "rule_id": rule["id"],
                    "cwe": rule["cwe"],
                    "description": rule["description"],
                    "severity": rule["severity"],
                    "matched_key": key,
                    "matched_value": value
                })
    return matches

# function
def rule_based_analysis_on_ml_aug():
    rules = load_rules(RULES_PATH)
    results = {}
    summary = defaultdict(lambda: {"vulnerable": 0, "non-vulnerable": 0})
    cve_counter = defaultdict(int)

    for category in os.listdir(INPUT_DIR):
        cat_path = os.path.join(INPUT_DIR, category)
        if not os.path.isdir(cat_path):
            continue

        for file in os.listdir(cat_path):
            if not file.endswith('.yaml'):
                continue
            file_path = os.path.join(cat_path, file)
            with open(file_path) as f:
                try:
                    content = yaml.safe_load(f) or {}
                except:
                    continue

            matched_rules = []
            for rule in rules:
                if rule['id'].lower().startswith(category[:3]):
                    matched = match_rule(content, rule)
                    matched_rules.extend(matched)
                    for m in matched:
                        cve_counter[m["cwe"]] += 1

            label = 'vulnerable' if matched_rules else 'non-vulnerable'
            results[file] = {
                "category": category,
                "label": label,
                "matched_rules": matched_rules
            }
            summary[category][label] += 1

    # output
    with open(RESULT_YAML, 'w') as f:
        yaml.dump(results, f)

    with open(RESULT_TXT, 'w') as f:
        for cat, stats in summary.items():
            f.write(f"{cat} - Vulnerable: {stats['vulnerable']}, Non-Vulnerable: {stats['non-vulnerable']}\n")

    # Save CWE frequency CSV
    df_cwe = pd.DataFrame(list(cve_counter.items()), columns=["CWE", "Count"])
    df_cwe.to_csv(CVE_CSV, index=False)

    # Pie Charts per Category
    for cat, stat in summary.items():
        labels = ['Vulnerable', 'Non-Vulnerable']
        values = [stat['vulnerable'], stat['non-vulnerable']]
        plt.figure(figsize=(5, 5))
        plt.pie(values, labels=labels,
                autopct=lambda p: f'{p:.1f}%\n({int(p * sum(values) / 100)})',
                colors=['red', 'green'])
        plt.title(f"{cat.title()} - Vulnerability Distribution")
        plt.savefig(f"{PIE_BASE}{cat}.png")
        plt.close()

    # Bar Chart for CWE Frequency
    if cve_counter:
        sorted_cve = dict(sorted(cve_counter.items(), key=lambda item: item[1], reverse=True))
        plt.figure(figsize=(12, 6))
        plt.bar(sorted_cve.keys(), sorted_cve.values(), color='skyblue')
        plt.xticks(rotation=45)
        for i, (cwe, count) in enumerate(sorted_cve.items()):
            plt.text(i, count + 1, str(count), ha='center', fontsize=8)
        plt.title("ML-Augmented - CWE Rule Match Frequency")
        plt.xlabel("CWE ID")
        plt.ylabel("Match Count")
        plt.tight_layout()
        plt.savefig(BAR_PLOT)
        plt.close()

    print(" Rule Analysis completed and saved.")

#  Run
rule_based_analysis_on_ml_aug()

import yaml
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import os

# Load ground truth labels
def load_ground_truth(metadata_file):
    with open(metadata_file, 'r') as f:
        metadata = yaml.safe_load(f)
    return {fname: info['label'] for fname, info in metadata.items()}

# Load predicted labels
def load_predictions(result_file):
    with open(result_file, 'r') as f:
        predictions = yaml.safe_load(f)
    return {fname: info['label'] for fname, info in predictions.items()}

# Evaluate model performance
def evaluate_accuracy(metadata_file, result_file):
    ground_truth = load_ground_truth(metadata_file)
    predicted_labels = load_predictions(result_file)

    y_true = []
    y_pred = []

    for fname in ground_truth:
        if fname in predicted_labels:
            y_true.append(ground_truth[fname])
            y_pred.append(predicted_labels[fname])
        else:
            print(f" Missing prediction for: {fname}")

    print(" Classification Report:\n")
    print(classification_report(y_true, y_pred, target_names=['non-vulnerable', 'vulnerable']))

    print(" Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))

    print(f"\n Accuracy: {accuracy_score(y_true, y_pred):.4f}")


metadata_file = "/content/drive/MyDrive/MisconfAI/merged_metadata.yaml"
result_file = "/content/drive/MyDrive/MisconfAI/new/rule_analysis_results.txt"

# Run
evaluate_accuracy(metadata_file, result_file)

import os
import yaml
import re
import matplotlib.pyplot as plt
from collections import defaultdict

# Load rules
def load_rules(rules_path):
    with open(rules_path, "r") as f:
        return yaml.safe_load(f)['rules']

# Load metadata
def load_metadata(metadata_path):
    with open(metadata_path, 'r') as f:
        return yaml.safe_load(f)

# Match
def match_rule(content, rule):
    matched_keys = []
    for key in rule.get("keys", []):
        if key in content:
            value = str(content[key])
            if re.search(rule["value_regex"], value):
                matched_keys.append(key)
    return matched_keys

def rule_based_analysis(
    rules_path="/content/drive/MyDrive/MisconfAI/rules.yaml",
    metadata_path="/content/drive/MyDrive/MisconfAI/merged_metadata.yaml",
    base_path="/content/drive/MyDrive/MisconfAI/ml_augmented_configs"
):
    import matplotlib.pyplot as plt
    import yaml, os, re
    from collections import defaultdict

    rules = load_rules(rules_path)
    metadata = load_metadata(metadata_path)

    result_yaml = {}
    cve_count = defaultdict(int)
    category_summary = defaultdict(lambda: {"vulnerable": 0, "non-vulnerable": 0})

    folder_category_map = {
        "application": "APP",
        "webserver": "WS",
        "cloud": "CLOUD",
        "database": "DB",
        "devops": "DEVOPS"
    }

    for category in folder_category_map:
        cat_path = os.path.join(base_path, category)
        rule_set = [r for r in rules if r["id"].startswith(folder_category_map[category])]

        for file_name in os.listdir(cat_path):
            file_path = os.path.join(cat_path, file_name)

            try:
                with open(file_path, 'r') as f:
                    content = yaml.safe_load(f) or {}
            except:
                continue

            # Label from metadata
            meta = metadata.get(file_name, {})
            label = meta.get("label", "non-vulnerable")
            matched_rules = []

            for rule in rule_set:
                matched_keys = match_rule(content, rule)
                if matched_keys:
                    matched_rules.append({
                        "rule_id": rule["id"],
                        "cwe": rule["cwe"],
                        "description": rule["description"],
                        "severity": rule["severity"],
                        "matched_keys": matched_keys
                    })
                    cve_count[rule["cwe"]] += 1


            if matched_rules:
                label = "vulnerable"

            result_yaml[file_name] = {
                "category": category,
                "label": label,
                "matched_rules": matched_rules
            }

            category_summary[category][label] += 1

    # Save YAML result
    with open("/content/drive/MyDrive/MisconfAI/rule_based_results.yaml", "w") as f:
        yaml.dump(result_yaml, f)

    # Save text summary
    with open("/content/drive/MyDrive/MisconfAI/rule_analysis_results.txt", "w") as f:
        for cat, stats in category_summary.items():
            f.write(f"[{cat.upper()}] - Vulnerable: {stats['vulnerable']} | Non-Vulnerable: {stats['non-vulnerable']}\n")
        f.write("\nCVE Match Count:\n")
        for cwe, count in cve_count.items():
            f.write(f"{cwe}: {count}\n")

    # Save CSV count
    with open("/content/drive/MyDrive/MisconfAI/cve_counts.csv", "w") as f:
        f.write("CWE,Count\n")
        for cwe, count in cve_count.items():
            f.write(f"{cwe},{count}\n")

    # PIE CHARTS for each category
    for cat in category_summary:
        labels = ['vulnerable', 'non-vulnerable']
        values = [
            category_summary[cat].get('vulnerable', 0),
            category_summary[cat].get('non-vulnerable', 0)
        ]
        plt.figure(figsize=(4, 4))
        plt.pie(values, labels=labels, autopct='%1.1f%%', colors=['red', 'green'])
        plt.title(f"{cat.capitalize()} Files - Vulnerability Distribution")
        plt.savefig(f"{cat}_pie_chart.png")
        plt.close()

    # BAR CHART of CWE count
    plt.figure(figsize=(10, 6))
    plt.bar(cve_count.keys(), cve_count.values(), color='purple')
    plt.xticks(rotation=45, ha="right")
    plt.title("CVE Frequency Across All Categories")
    plt.xlabel("CWE ID")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig("cve_bar_chart.png")
    plt.close()

    print(" Rule-based analysis completed.")
    print("- YAML: rule_based_results.yaml")
    print("- Summary: rule_analysis_results.txt")
    print("- CVE Count CSV: cve_counts.csv")
    print("- Pie Charts & Bar Chart saved.")

rule_based_analysis()

"""**calculate accuracy**

**Converting metadata into csv**
"""

import os
import yaml
import pandas as pd
from pathlib import Path

def flatten_yaml(yaml_obj):

    flat_lines = []
    def recurse(d, parent_key=""):
        if isinstance(d, dict):
            for k, v in d.items():
                new_key = f"{parent_key}.{k}" if parent_key else k
                recurse(v, new_key)
        else:
            flat_lines.append(f"{parent_key}: {d}")
    recurse(yaml_obj)
    return "\n".join(flat_lines)

def convert_metadata_to_csv(metadata_path, config_root, output_csv_path):
    with open(metadata_path, 'r') as f:
        metadata = yaml.safe_load(f)

    records = []
    for filename, meta in metadata.items():
        category = meta.get("category")
        full_path = os.path.join(config_root, category, filename)

        if not os.path.exists(full_path):
            continue

        try:
            with open(full_path, 'r') as cf:
                content = yaml.safe_load(cf)
            text = flatten_yaml(content)
        except:
            continue

        cwe_ids = []
        for rule_id in meta.get("injected_rules", []):
            if "CWE-" in rule_id:
                cwe_ids.append(rule_id.split("CWE-")[-1])
            elif rule_id.startswith("CWE"):
                cwe_ids.append(rule_id[3:])

        records.append({
            "filename": filename,
            "text": text,
            "label": 1 if meta["label"] == "vulnerable" else 0,
            "category": category,
            "type": meta.get("type"),
            "injected_rules": ",".join(meta.get("injected_rules", [])),
            "cwe_ids": ",".join(cwe_ids)
        })

    df = pd.DataFrame(records)
    df.to_csv(output_csv_path, index=False)
    print(f"CSV saved to {output_csv_path}")
    return df

csv_df = convert_metadata_to_csv(
    metadata_path="/content/drive/MyDrive/MisconfAI/merged_metadata.yaml",
    config_root="/content/drive/MyDrive/MisconfAI/ml_augmented_configs",
    output_csv_path="/content/drive/MyDrive/MisconfAI/metadata.csv"
)

"""**in the above it was not giving cwe for all 1 vulnerable files**"""

import os
import yaml
import pandas as pd
from pathlib import Path

def flatten_yaml(yaml_obj):

    flat_lines = []
    def recurse(d, parent_key=""):
        if isinstance(d, dict):
            for k, v in d.items():
                new_key = f"{parent_key}.{k}" if parent_key else k
                recurse(v, new_key)
        else:
            flat_lines.append(f"{parent_key}: {d}")
    recurse(yaml_obj)
    return "\n".join(flat_lines)

def convert_augmented_metadata_to_csv(metadata_path, config_root, output_csv_path):
    with open(metadata_path, 'r') as f:
        metadata = yaml.safe_load(f)

    records = []
    for filename, meta in metadata.items():
        category = meta.get("category")
        full_path = os.path.join(config_root, category, filename)

        if not os.path.exists(full_path):
            continue

        try:
            with open(full_path, 'r') as cf:
                content = yaml.safe_load(cf)
            text = flatten_yaml(content)
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            continue

        # Extract CWE
        cwe_ids = []
        if isinstance(meta.get("vulnerable_rules"), list):
            for rule in meta["vulnerable_rules"]:
                if isinstance(rule, dict) and "cwe" in rule:
                    cwe_ids.append(rule["cwe"])

        records.append({
            "filename": filename,
            "text": text,
            "label": 1 if meta.get("label") == "vulnerable" else 0,
            "category": category,
            "type": meta.get("type"),
            "cwe_ids": ",".join(cwe_ids)
        })

    df = pd.DataFrame(records)
    df.to_csv(output_csv_path, index=False)
    print(f"CSV saved to: {output_csv_path}")
    return df


csv_df = convert_augmented_metadata_to_csv(
    metadata_path="/content/drive/MyDrive/MisconfAI/augmented_configs/metadata.yaml",
    config_root="/content/drive/MyDrive/MisconfAI/augmented_configs",
    output_csv_path="/content/drive/MyDrive/MisconfAI/metadata.csv"
)

"""**csv file from raugmented_data**"""

import os
import yaml
import pandas as pd
from pathlib import Path

def flatten_yaml(yaml_obj):
    flat_lines = []
    def recurse(d, parent_key=""):
        if isinstance(d, dict):
            for k, v in d.items():
                new_key = f"{parent_key}.{k}" if parent_key else k
                recurse(v, new_key)
        else:
            flat_lines.append(f"{parent_key}: {d}")
    recurse(yaml_obj)
    return "\n".join(flat_lines)

def convert_augmented_metadata_to_csv(metadata_path, config_root, output_csv_path):
    with open(metadata_path, 'r') as f:
        metadata = yaml.safe_load(f)

    records = []
    for filename, meta in metadata.items():
        category = meta.get("category")
        full_path = os.path.join(config_root, category, filename)

        if not os.path.exists(full_path):
            continue

        try:
            with open(full_path, 'r') as cf:
                content = yaml.safe_load(cf)
            text = flatten_yaml(content)
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            continue


        cwe_ids = []
        if isinstance(meta.get("vulnerable_rules"), list):
            for rule in meta["vulnerable_rules"]:
                if isinstance(rule, dict) and "cwe" in rule:
                    cwe_ids.append(rule["cwe"])

        records.append({
            "filename": filename,
            "text": text,
            "label": 1 if meta.get("label") == "vulnerable" else 0,
            "category": category,
            "cwe_ids": ",".join(cwe_ids)
        })

    df = pd.DataFrame(records)
    df.to_csv(output_csv_path, index=False)
    print(f"CSV saved to: {output_csv_path}")
    return df


csv_df = convert_augmented_metadata_to_csv(
    metadata_path="/content/drive/MyDrive/MisconfAI/rule_augmented_configs/metadata.yaml",
    config_root="/content/drive/MyDrive/MisconfAI/rule_augmented_configs",
    output_csv_path="/content/drive/MyDrive/MisconfAI/fmetadata.csv"
)

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/MisconfAI/fmetadata.csv')
print("Shape:", df.shape)

"""**final metadata for final augmentation**"""

import os
import yaml
import pandas as pd
from pathlib import Path

def flatten_yaml(yaml_obj):
    flat_lines = []
    def recurse(d, parent_key=""):
        if isinstance(d, dict):
            for k, v in d.items():
                new_key = f"{parent_key}.{k}" if parent_key else k
                recurse(v, new_key)
        else:
            flat_lines.append(f"{parent_key}: {d}")
    recurse(yaml_obj)
    return "\n".join(flat_lines)

def convert_augmented_metadata_to_csv(metadata_path, config_root, output_csv_path):
    with open(metadata_path, 'r') as f:
        metadata = yaml.safe_load(f)

    records = []
    for filename, meta in metadata.items():
        category = meta.get("category")
        full_path = os.path.join(config_root, category, filename)

        if not os.path.exists(full_path):
            continue

        try:
            with open(full_path, 'r') as cf:
                content = yaml.safe_load(cf)
            text = flatten_yaml(content)
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            continue


        cwe_ids = []
        if isinstance(meta.get("vulnerable_rules"), list):
            for rule in meta["vulnerable_rules"]:
                if isinstance(rule, dict) and "cwe" in rule:
                    cwe_ids.append(rule["cwe"])

        records.append({
            "filename": filename,
            "text": text,
            "label": 1 if meta.get("label") == "vulnerable" else 0,
            "category": category,
            "cwe_ids": ",".join(cwe_ids)
        })

    df = pd.DataFrame(records)
    df.to_csv(output_csv_path, index=False)
    print(f"CSV saved to: {output_csv_path}")
    return df


csv_df = convert_augmented_metadata_to_csv(
    metadata_path="/content/drive/MyDrive/MisconfAI/finalaugmented_configs/metadata.yaml",
    config_root="/content/drive/MyDrive/MisconfAI/finalaugmented_configs",
    output_csv_path="/content/drive/MyDrive/MisconfAI/finmetadata.csv"
)

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/MisconfAI/finmetadata.csv')
print("Shape:", df.shape)

"""**in this text was missing**"""

import yaml
import os
import pandas as pd

# Paths
metadata_path = "/content/drive/MyDrive/MisconfAI/rule_augmented_configs/metadata.yaml"
file_root_path = "/content/drive/MyDrive/MisconfAI/rule_augmented_configs"

# Load metadata.yaml
with open(metadata_path, 'r') as f:
    metadata = yaml.safe_load(f)

data = []

for filename, info in metadata.items():
    filepath = os.path.join(file_root_path, filename)
    try:
        with open(filepath, 'r') as f:
            file_text = f.read()
    except FileNotFoundError:
        file_text = ""
        print(f"Missing file: {filename}")

    label = 1 if info["label"] == "vulnerable" else 0
    category = info.get("category", "unknown")
    cwe_ids = list(set(info.get("injected_rules", [])))

    data.append({
        "filename": filename,
        "text": file_text,
        "label": label,
        "category": category,
        "cwe_id": ";".join(cwe_ids) if cwe_ids else "none"
    })

# Convert to DataFrame
df = pd.DataFrame(data)

# Save to CSV
csv_output_path = "/content/drive/MyDrive/MisconfAI/metadata_final.csv"
df.to_csv(csv_output_path, index=False)

print(f"CSV saved to: {csv_output_path}")

"""**Now NLP models**

**1. BERT**
"""

!pip install datasets

!pip install -U transformers

!pip install -U transformers datasets

"""**Category wise prediction and model prediction**

**BERT**
"""

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import DataCollatorWithPadding
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
import os

# 1. Load CSV
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
output_dir="/content/drive/MyDrive/MisconfAI/bert"
df = pd.read_csv(csv_path)

# 2. Keep columns
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

# 3. Train-test split
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)

# 4. Save true categories for evaluation
true_categories = test_df["category"].values
true_labels = test_df["label"].values

# 5. Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# 6. Tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# 7. Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)

train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

# 8. Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 9. Metrics function
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# 10. Training args
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

# 11. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# 12. Train
trainer.train()

# 13. Evaluate basic metrics
metrics = trainer.evaluate()
print("Evaluation metrics:", metrics)

# 14. Saving the model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# 15. Custom category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)

test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds


category_stats = {}

categories = test_df["category"].unique()
for category in sorted(categories):
    actual_vuln = test_df[(test_df["category"] == category) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]

    category_stats[category] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_bert": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

# 16. Print results as table
print("\nCategory-wise Vulnerability Detection by BERT:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by BERT", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_bert"], stats["accuracy_on_vulnerable"]))

"""**Codebert**"""

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
import os

# 1. Load CSV
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
output_dir="/content/drive/MyDrive/MisconfAI/codebert"
df = pd.read_csv(csv_path)

# 2. Keep columns
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

# 3. Train-test split
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=45)

# 4. Save true categories for evaluation
true_categories = test_df["category"].values
true_labels = test_df["label"].values

# 5. Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# 6. Tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
model = RobertaForSequenceClassification.from_pretrained("microsoft/codebert-base", num_labels=2)

# 7. Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)

train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

# 8. Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 9. Metrics function
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# 10. Training args
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

# 11. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# 12. Train
trainer.train()

# 13. Evaluate basic metrics
metrics = trainer.evaluate()
print("Evaluation metrics:", metrics)

# 14. Saving the model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# 15. Custom category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)

test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds


category_stats = {}

categories = test_df["category"].unique()
for category in sorted(categories):
    actual_vuln = test_df[(test_df["category"] == category) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]

    category_stats[category] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_bert": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

# 16. Print results as table
print("\nCategory-wise Vulnerability Detection by CODEBERT:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by CODEBERT", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_bert"], stats["accuracy_on_vulnerable"]))

"""**Roberta**"""

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import DataCollatorWithPadding
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
import os

# 1. Load CSV
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
output_dir="/content/drive/MyDrive/MisconfAI/roberta"
df = pd.read_csv(csv_path)

# 2. Keep columns
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

# 3. Train-test split
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=46)

# 4. Save true categories for evaluation
true_categories = test_df["category"].values
true_labels = test_df["label"].values

# 5. Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# 6. Tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=2)

# 7. Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)

train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

# 8. Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 9. Metrics function
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# 10. Training args
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

# 11. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# 12. Train
trainer.train()

# 13. Evaluate basic metrics
metrics = trainer.evaluate()
print("Evaluation metrics:", metrics)

# 14. Saving the model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# 15. Custom category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)

test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds


category_stats = {}

categories = test_df["category"].unique()
for category in sorted(categories):
    actual_vuln = test_df[(test_df["category"] == category) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]

    category_stats[category] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_bert": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

# 16. Print results as table
print("\nCategory-wise Vulnerability Detection by ROBERTA:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by RoBERTa", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_bert"], stats["accuracy_on_vulnerable"]))

"""**ELECTRA**"""

import pandas as pd
import torch
from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import DataCollatorWithPadding
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import numpy as np
import os

# Load and preprocess
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
df = pd.read_csv(csv_path)
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=43)
true_categories = test_df["category"].values

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Tokenizer & model
model_name = "google/electra-base-discriminator"
tokenizer = ElectraTokenizer.from_pretrained(model_name)
model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)
train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Metrics
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# Training setup
output_dir = "/content/drive/MyDrive/MisconfAI/electra"
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

trainer = Trainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Save model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# Evaluate
metrics = trainer.evaluate()
print("ELECTRA Evaluation metrics:", metrics)

# Category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)
test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds

category_stats = {}
for cat in sorted(test_df["category"].unique()):
    actual_vuln = test_df[(test_df["category"] == cat) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]
    category_stats[cat] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_electra": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

print("\nCategory-wise Vulnerability Detection by ELECTRA:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by ELECTRA", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_electra"], stats["accuracy_on_vulnerable"]))

"""**XLNet**"""

from transformers import XLNetTokenizer, XLNetForSequenceClassification

# Load and preprocess
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
df = pd.read_csv(csv_path)
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=44)
true_categories = test_df["category"].values

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Tokenizer & model
model_name = "xlnet-base-cased"
tokenizer = XLNetTokenizer.from_pretrained(model_name)
model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True, max_length=256)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)
train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Metrics
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}


# Training setup
output_dir = "/content/drive/MyDrive/MisconfAI/xlnet"
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

trainer = Trainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Save model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# Evaluate
metrics = trainer.evaluate()
print("XLNet Evaluation metrics:", metrics)

# Category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)
test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds

category_stats = {}
for cat in sorted(test_df["category"].unique()):
    actual_vuln = test_df[(test_df["category"] == cat) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]
    category_stats[cat] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_xlnet": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

print("\nCategory-wise Vulnerability Detection by XLNet:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by XLNet", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_xlnet"], stats["accuracy_on_vulnerable"]))

"""**Rule analysis on final_aug**"""

import os, yaml, re
from collections import defaultdict
import pandas as pd

# Config
RULES_PATH = '/content/drive/MyDrive/MisconfAI/rules.yaml'
INPUT_DIR = '/content/drive/MyDrive/MisconfAI/finalaugmented_configs'
RESULT_YAML = '/content/drive/MyDrive/MisconfAI/1.rule_based_results.yaml'
RESULT_TXT = '/content/drive/MyDrive/MisconfAI/1.rule_analysis_results.txt'
CVE_CSV = '/content/drive/MyDrive/MisconfAI/1.rule_based_cve_counts.csv'

# YAML Rules
def load_rules(path):
    with open(path) as f:
        return yaml.safe_load(f)['rules']

# Match
def match_rule(content, rule):
    matched = []
    for key in rule.get('keys', []):
        if key in content:
            value = str(content[key])
            if re.search(rule['value_regex'], value):
                matched.append({
                    'rule_id': rule['id'],
                    'cwe': rule['cwe'],
                    'description': rule['description'],
                    'severity': rule['severity'],
                    'matched_key': key,
                    'matched_value': value
                })
    return matched

# Rule-Based Analysis
def rule_based_analysis():
    rules = load_rules(RULES_PATH)
    results = {}
    cve_counter = defaultdict(int)
    summary = defaultdict(lambda: {'vulnerable': 0, 'non-vulnerable': 0})

    category_map = {
        "application": "APP",
        "webserver": "WS",
        "cloud": "CLOUD",
        "database": "DB",
        "devops": "DEVOPS"
    }

    for category in category_map:
        cat_path = os.path.join(INPUT_DIR, category)
        rule_set = [r for r in rules if r['id'].startswith(category_map[category])]
        for fname in os.listdir(cat_path):
            if not fname.endswith(".yaml"):
                continue
            path = os.path.join(cat_path, fname)
            try:
                with open(path) as f:
                    content = yaml.safe_load(f) or {}
            except:
                continue

            matched = []
            for rule in rule_set:
                matched.extend(match_rule(content, rule))
                for m in matched:
                    cve_counter[m['cwe']] += 1

            label = 'vulnerable' if matched else 'non-vulnerable'
            results[fname] = {
                'category': category,
                'label': label,
                'matched_rules': matched
            }
            summary[category][label] += 1

    with open(RESULT_YAML, 'w') as f:
        yaml.dump(results, f)

    with open(RESULT_TXT, 'w') as f:
        for cat, stat in summary.items():
            f.write(f"{cat.upper()} - Vulnerable: {stat['vulnerable']}, Non-Vulnerable: {stat['non-vulnerable']}\n")
        f.write("\nCVE Match Count:\n")
        for cwe, count in cve_counter.items():
            f.write(f"{cwe}: {count}\n")

    pd.DataFrame(list(cve_counter.items()), columns=["CWE", "Count"]).to_csv(CVE_CSV, index=False)
    print(" Rule-based analysis completed.")

rule_based_analysis()

import yaml
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Paths
GROUND_TRUTH = '/content/drive/MyDrive/MisconfAI/finalaugmented_configs/metadata.yaml'
PREDICTIONS = '/content/drive/MyDrive/MisconfAI/1.rule_based_results.yaml'

# Load
with open(GROUND_TRUTH) as f:
    gt = yaml.safe_load(f)
with open(PREDICTIONS) as f:
    pred = yaml.safe_load(f)

# Match
y_true, y_pred = [], []

for fname in gt:
    true = 1 if gt[fname]['label'] == 'vulnerable' else 0
    predicted = 1 if pred.get(fname, {}).get('label') == 'vulnerable' else 0
    y_true.append(true)
    y_pred.append(predicted)

# Metrics
print("Evaluation Metrics (Rule-Based vs Ground Truth)")
print("Accuracy:", accuracy_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred, zero_division=0))
print("Recall:", recall_score(y_true, y_pred, zero_division=0))
print("F1 Score:", f1_score(y_true, y_pred, zero_division=0))

"""**Running all the models again for fix input**

**2. BERT**
"""

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import DataCollatorWithPadding
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
import os

# 1. Load CSV
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
output_dir="/content/drive/MyDrive/MisconfAI/2.bert"
df = pd.read_csv(csv_path)

# 2. Keep columns
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

# 3. Train-test split
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)

# 4. Save true categories for evaluation
true_categories = test_df["category"].values
true_labels = test_df["label"].values

# 5. Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# 6. Tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# 7. Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)

train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

# 8. Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 9. Metrics function
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# 10. Training args
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

# 11. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# 12. Train
trainer.train()

# 13. Evaluate basic metrics
metrics = trainer.evaluate()
print("Evaluation metrics:", metrics)

# 14. Saving the model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# 15. Custom category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)

test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds


category_stats = {}

categories = test_df["category"].unique()
for category in sorted(categories):
    actual_vuln = test_df[(test_df["category"] == category) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]

    category_stats[category] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_bert": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

# 16. Print results as table
print("\nCategory-wise Vulnerability Detection by BERT:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by BERT", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_bert"], stats["accuracy_on_vulnerable"]))

"""**2. CodeBERT**"""

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
import os

# 1. Load CSV
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
output_dir="/content/drive/MyDrive/MisconfAI/2.codebert"
df = pd.read_csv(csv_path)

# 2. Keep necessary columns
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

# 3. Train-test split
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)

# 4. Save true categories for evaluation
true_categories = test_df["category"].values
true_labels = test_df["label"].values

# 5. Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# 6. Tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
model = RobertaForSequenceClassification.from_pretrained("microsoft/codebert-base", num_labels=2)

# 7. Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)

train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

# 8. Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 9. Metrics function
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# 10. Training args
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

# 11. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# 12. Train
trainer.train()

# 13. Evaluate basic metrics
metrics = trainer.evaluate()
print("Evaluation metrics:", metrics)

# 14. Saving the model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# 15. Custom category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)

test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds

category_stats = {}

categories = test_df["category"].unique()
for category in sorted(categories):
    actual_vuln = test_df[(test_df["category"] == category) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]

    category_stats[category] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_bert": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

# 16. Print results as table
print("\nCategory-wise Vulnerability Detection by CODEBERT:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by CODEBERT", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_bert"], stats["accuracy_on_vulnerable"]))

"""**2. RoBERTa**"""

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import DataCollatorWithPadding
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix
import os

# 1. Load CSV
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
output_dir="/content/drive/MyDrive/MisconfAI/2.roberta"
df = pd.read_csv(csv_path)

# 2. Keep necessary columns
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

# 3. Train-test split
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)

# 4. Save true categories for evaluation
true_categories = test_df["category"].values
true_labels = test_df["label"].values

# 5. Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# 6. Tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=2)

# 7. Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)

train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

# 8. Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 9. Metrics function
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# 10. Training args
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

# 11. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# 12. Train
trainer.train()

# 13. Evaluate basic metrics
metrics = trainer.evaluate()
print("Evaluation metrics:", metrics)

# 14. Saving the model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# 15. Custom category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)

test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds

category_stats = {}

categories = test_df["category"].unique()
for category in sorted(categories):
    actual_vuln = test_df[(test_df["category"] == category) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]

    category_stats[category] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_bert": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

# 16. Print results as table
print("\nCategory-wise Vulnerability Detection by RoBERTa:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by RoBERTa", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_bert"], stats["accuracy_on_vulnerable"]))

"""**2. ELECTRA**"""

import pandas as pd
import torch
from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import DataCollatorWithPadding
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import numpy as np
import os

# Load and preprocess
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
df = pd.read_csv(csv_path)
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)
true_categories = test_df["category"].values

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Tokenizer & model
model_name = "google/electra-base-discriminator"
tokenizer = ElectraTokenizer.from_pretrained(model_name)
model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)
train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Metrics
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# Training setup
output_dir = "/content/drive/MyDrive/MisconfAI/2.electra"
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

trainer = Trainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Save model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# Evaluate
metrics = trainer.evaluate()
print("ELECTRA Evaluation metrics:", metrics)

# Category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)
test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds

category_stats = {}
for cat in sorted(test_df["category"].unique()):
    actual_vuln = test_df[(test_df["category"] == cat) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]
    category_stats[cat] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_electra": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

print("\nCategory-wise Vulnerability Detection by ELECTRA:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by ELECTRA", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_electra"], stats["accuracy_on_vulnerable"]))

"""**2. XLNet**"""

from transformers import XLNetTokenizer, XLNetForSequenceClassification

# Load and preprocess
csv_path = "/content/drive/MyDrive/MisconfAI/finmetadata.csv"
df = pd.read_csv(csv_path)
df = df[["text", "label", "category"]]
df["text"] = df["text"].astype(str)
df["category"] = df["category"].astype(str)

train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)
true_categories = test_df["category"].values

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Tokenizer & model
model_name = "xlnet-base-cased"
tokenizer = XLNetTokenizer.from_pretrained(model_name)
model = XLNetForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenization
def tokenize_fn(batch):
    return tokenizer(batch["text"], padding=True, truncation=True, max_length=256)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)
train_dataset = train_dataset.remove_columns(["text", "category"])
test_dataset = test_dataset.remove_columns(["text", "category"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Metrics
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}


# Training setup
output_dir = "/content/drive/MyDrive/MisconfAI/2.xlnet"
training_args = TrainingArguments(
    output_dir=output_dir,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to=[]
)

trainer = Trainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Save model
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# Evaluate
metrics = trainer.evaluate()
print("XLNet Evaluation metrics:", metrics)

# Category-wise analysis
preds_raw = trainer.predict(test_dataset)
preds = np.argmax(preds_raw.predictions, axis=1)
test_df = test_df.reset_index(drop=True)
test_df["predicted_label"] = preds

category_stats = {}
for cat in sorted(test_df["category"].unique()):
    actual_vuln = test_df[(test_df["category"] == cat) & (test_df["label"] == 1)]
    detected_vuln = actual_vuln[actual_vuln["predicted_label"] == 1]
    category_stats[cat] = {
        "actual_vulnerable": len(actual_vuln),
        "detected_by_xlnet": len(detected_vuln),
        "accuracy_on_vulnerable": round(len(detected_vuln) / len(actual_vuln), 2) if len(actual_vuln) > 0 else "-"
    }

print("\nCategory-wise Vulnerability Detection by XLNet:")
print("{:<15} {:<20} {:<20} {:<10}".format("Category", "Actual Vulnerable", "Detected by XLNet", "Accuracy"))
for cat, stats in category_stats.items():
    print("{:<15} {:<20} {:<20} {:<10}".format(cat, stats["actual_vulnerable"], stats["detected_by_xlnet"], stats["accuracy_on_vulnerable"]))